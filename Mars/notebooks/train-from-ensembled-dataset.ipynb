{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10169027,"sourceType":"datasetVersion","datasetId":6280090},{"sourceId":10170341,"sourceType":"datasetVersion","datasetId":6281086},{"sourceId":10180115,"sourceType":"datasetVersion","datasetId":6288304}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import libraries","metadata":{"id":"Ixw2GZ2kdv4c"}},{"cell_type":"code","source":"pip install segmentation_models_pytorch","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DVYq-2WRXXnb","outputId":"09c5061b-57d2-4172-a1de-7d9f93377165","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T16:44:16.617894Z","iopub.execute_input":"2024-12-12T16:44:16.618222Z","iopub.status.idle":"2024-12-12T16:44:31.740026Z","shell.execute_reply.started":"2024-12-12T16:44:16.618192Z","shell.execute_reply":"2024-12-12T16:44:31.738992Z"}},"outputs":[{"name":"stdout","text":"Collecting segmentation_models_pytorch\n  Downloading segmentation_models_pytorch-0.3.4-py3-none-any.whl.metadata (30 kB)\nCollecting efficientnet-pytorch==0.7.1 (from segmentation_models_pytorch)\n  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: huggingface-hub>=0.24.6 in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.26.2)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (10.3.0)\nCollecting pretrainedmodels==0.7.4 (from segmentation_models_pytorch)\n  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (1.16.0)\nCollecting timm==0.9.7 (from segmentation_models_pytorch)\n  Downloading timm-0.9.7-py3-none-any.whl.metadata (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.19.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from segmentation_models_pytorch) (4.66.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.4.0)\nCollecting munch (from pretrainedmodels==0.7.4->segmentation_models_pytorch)\n  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm==0.9.7->segmentation_models_pytorch) (6.0.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm==0.9.7->segmentation_models_pytorch) (0.4.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation_models_pytorch) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation_models_pytorch) (2024.6.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation_models_pytorch) (21.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation_models_pytorch) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation_models_pytorch) (4.12.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.26.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.24.6->segmentation_models_pytorch) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation_models_pytorch) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation_models_pytorch) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation_models_pytorch) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation_models_pytorch) (2024.6.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.3.0)\nDownloading segmentation_models_pytorch-0.3.4-py3-none-any.whl (109 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading timm-0.9.7-py3-none-any.whl (2.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\nBuilding wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16427 sha256=bd6d1dce466956c578c04269fca654e197bbecb47a83f556a8a6e7a718f37bac\n  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=9efc8881e0783c96d2d4f74bced13239546c69a9007c0f77aa5df10f54c8b520\n  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\nSuccessfully built efficientnet-pytorch pretrainedmodels\nInstalling collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation_models_pytorch\n  Attempting uninstall: timm\n    Found existing installation: timm 1.0.11\n    Uninstalling timm-1.0.11:\n      Successfully uninstalled timm-1.0.11\nSuccessfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation_models_pytorch-0.3.4 timm-0.9.7\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport os\nimport numpy as np\nimport cv2\n\nimport math\nfrom torch.utils.data import DataLoader, distributed\nimport torch\n\nfrom typing import Dict, Union, Tuple, List\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\nfrom transformers import SegformerForSemanticSegmentation, SegformerConfig\nfrom torch.cuda import amp\n\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2","metadata":{"id":"RIFbAbZHXXnd","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import segmentation_models_pytorch as smp","metadata":{"id":"_JLnA8lqatqu","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T16:44:49.810289Z","iopub.execute_input":"2024-12-12T16:44:49.810765Z","iopub.status.idle":"2024-12-12T16:44:52.201126Z","shell.execute_reply.started":"2024-12-12T16:44:49.810738Z","shell.execute_reply":"2024-12-12T16:44:52.200381Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Load date from NP","metadata":{"id":"Aeh0pOhVd0Q4"}},{"cell_type":"code","source":"data = np.load(\"/kaggle/input/ensembled_dataset.npz\")\nX = data['dataset']\ny = data['masks']","metadata":{"id":"DkikpofBXXne","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T11:47:06.808827Z","iopub.execute_input":"2024-12-12T11:47:06.809633Z","iopub.status.idle":"2024-12-12T11:47:09.918171Z","shell.execute_reply.started":"2024-12-12T11:47:06.809595Z","shell.execute_reply":"2024-12-12T11:47:09.917075Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"X = np.squeeze(X)\nprint('X.shape:', X.shape)\nprint('y.shape:', y.shape)","metadata":{"id":"fYsiufBsA56x","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### encode_label_from_label_map\n\n- **Input**:  \n  - `label` (np.ndarray): A color image (H, W, 3) representing segmentation labels.  \n  - `label_map` (Dict): A dictionary mapping class names to their corresponding RGB colors.  \n- **Output**:  \n  - `np.ndarray`: A one-hot encoded mask of shape (H, W, n_classes).  \n- **Usage**: Converts a color-coded label into a one-hot encoded mask for semantic segmentation.  \n\n\n### decode_mask_from_color_map\n\n- **Input**:  \n  - `mask` (np.ndarray or torch.Tensor): A one-hot encoded mask of shape (H, W, n_classes).  \n  - `color_map` (Dict): A dictionary mapping class names to their corresponding RGB colors.  \n- **Output**:  \n  - `np.ndarray`: A decoded color image of shape (H, W, 3).  \n- **Usage**: Converts a one-hot encoded mask back into a color-coded label for visualization.  ","metadata":{"id":"ZsBsS_0sXXne"}},{"cell_type":"code","source":"def encode_label_from_label_map(label: np.ndarray, label_map: Dict) -> np.ndarray:\n    def multy_along_three_axis(a1, a2, a3):\n        return a1 * a2 * a3\n\n    n_classes = len(label_map.keys())\n    h, w = label.shape[:2]\n    mask = np.zeros((h, w, n_classes), dtype=np.float32)\n    for idx, cls in enumerate(label_map.keys()):\n        color = label_map[cls]\n        ij = label == color\n        if ij.ndim == 3:\n            ij = multy_along_three_axis(*ij.transpose(2, 0, 1))\n        mask[ij, idx] = 1\n\n    return mask\n\ndef decode_mask_from_color_map(mask, color_map: Dict):\n    if isinstance(torch.tensor, type(mask)):\n        mask = mask.permute(1, 2, 0).numpy()\n\n    h, w = mask.shape[:2]\n    label = np.zeros((h, w, 3), dtype=np.uint8)\n    for idx, cls in enumerate(color_map.keys()):\n        color = color_map[cls]\n        label[mask[..., idx] == 1] = color\n\n    return label","metadata":{"id":"4CoWAGcQXXnf","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T11:47:15.243238Z","iopub.execute_input":"2024-12-12T11:47:15.244047Z","iopub.status.idle":"2024-12-12T11:47:15.251054Z","shell.execute_reply.started":"2024-12-12T11:47:15.244014Z","shell.execute_reply":"2024-12-12T11:47:15.250143Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"plt.imshow(y[6])","metadata":{"id":"wjB52fdKXXnf","colab":{"base_uri":"https://localhost:8080/","height":329},"outputId":"e07f9a89-93b7-4557-82f8-039d6b9a54ba","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T11:47:18.490723Z","iopub.execute_input":"2024-12-12T11:47:18.491390Z","iopub.status.idle":"2024-12-12T11:47:18.750442Z","shell.execute_reply.started":"2024-12-12T11:47:18.491359Z","shell.execute_reply":"2024-12-12T11:47:18.749560Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"<matplotlib.image.AxesImage at 0x7b49c8dca1a0>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"## Augmentation","metadata":{"id":"WDxWYljsdol_"}},{"cell_type":"code","source":"from typing import List\n\nimport cv2\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\ndef geom_augs() -> A.Compose:\n    return A.Compose(\n        [\n            A.OneOf(\n                [\n                    A.ShiftScaleRotate(\n                        shift_limit=0.0825, scale_limit=0.2, rotate_limit=10, value=0, border_mode=cv2.BORDER_CONSTANT\n                    ),\n                    A.Rotate(limit=(-10, 10)),\n                ]\n            ),\n            A.OneOf(\n                [\n                    A.VerticalFlip(),\n                    A.HorizontalFlip(),\n                ]\n            ),\n        ]\n    )\n\ndef values_augs() -> A.Compose:\n    return A.Compose(\n        [\n            A.OneOf(\n                [\n                    A.RandomBrightnessContrast(brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1)),\n                    A.ISONoise(),\n                    A.Blur(),\n                    A.CLAHE(),\n                    A.Sharpen(),\n                    A.PixelDropout(),\n                    A.AdvancedBlur(),\n                    A.Defocus(),\n                ],\n            ),\n            A.OneOf(\n                [\n                    A.HueSaturationValue(),\n                    A.OpticalDistortion(),\n                ]\n            ),\n        ]\n    )\n\ndef augs_pipeline() -> A.Compose:\n    return A.Compose(\n        [\n            values_augs(),\n            geom_augs(),\n        ]\n    )\n\n\ndef pre_processing_pipeline(image_size: List[int]) -> A.Compose:\n    return A.Compose([A.RandomResizedCrop(scale=(0.6, 1), height=image_size[0], width=image_size[1], p=0.5)])\n\n\ndef post_processing_pipeline(image_size: List[int]) -> A.Compose:\n    return A.Compose(\n        [\n            A.PadIfNeeded(min_height=image_size[0], min_width=image_size[1], value=0, border_mode=cv2.BORDER_CONSTANT),\n            A.RandomCrop(height=image_size[0], width=image_size[1], always_apply=True),\n        ]\n    )\n\n\ndef train_transforms(*args, **kwargs) -> A.Compose:\n    return A.Compose(\n        [\n            augs_pipeline(),\n        ]\n    )\n\n\ndef valid_transforms(*args, **kwargs) -> A.Compose:\n    return A.Compose(\n        [\n            augs_pipeline(),\n        ]\n    )","metadata":{"id":"kkS__4sbdp0u","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T11:47:21.146627Z","iopub.execute_input":"2024-12-12T11:47:21.147646Z","iopub.status.idle":"2024-12-12T11:47:21.613466Z","shell.execute_reply.started":"2024-12-12T11:47:21.147605Z","shell.execute_reply":"2024-12-12T11:47:21.612546Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.22 (you have 1.4.21). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# MarsSurface Class\n\n`__init__(self, mode: str, X, y, label_map_dict, color_map_dict, train_transforms, valid_transforms)`\nInitializes the dataset class for the Mars surface segmentation task.\n\n- **Input**:\n  - `mode` (str): Specifies the mode of the dataset, one of [\"train\", \"valid\", \"test\", \"debug\"].\n  - `X`: The input images (array or list-like structure).\n  - `y`: The corresponding labels for the images.\n  - `label_map_dict` (dict): A dictionary mapping class names to their labels.\n  - `color_map_dict` (dict): A dictionary mapping class names to their RGB color representation.\n  - `train_transforms` (callable): Transformations to be applied during training.\n  - `valid_transforms` (callable): Transformations to be applied during validation/testing.\n\n- **Output**: None\n\n\n`_read_data(self, item: int)`\nReads and preprocesses a single data point from the dataset.\n\n- **Input**:\n  - `item` (int): Index of the data point to read.\n\n- **Output**:\n  - `image` (np.ndarray): The preprocessed image.\n  - `label` (np.ndarray): The corresponding label encoded using `label_map_dict`.\n\n\n`_decode_label(self, label_map)`\nEncodes the label map into a one-hot encoded mask.\n\n- **Input**:\n  - `label_map` (np.ndarray): The label map for the given image.\n\n- **Output**:\n  - `mask` (np.ndarray): One-hot encoded mask of shape (H, W, n_classes).\n\n\n`__len__(self)`\nReturns the total number of samples in the dataset.\n\n- **Input**: None\n\n- **Output**:\n  - (int): Number of samples.\n\n\n`__getitem__(self, item: int)`\nFetches a data point, applies transformations, and prepares it for training or evaluation.\n\n- **Input**:\n  - `item` (int): Index of the data point.\n\n- **Output**:\n  - `image` (torch.Tensor): The transformed image tensor of shape (3, H, W).\n  - `mask` (torch.Tensor): The one-hot encoded mask tensor of shape (n_classes, H, W).\n  - `zero_mask` (torch.Tensor): A mask highlighting unannotated areas of shape (n_classes, H, W).\n  - `_label` (torch.Tensor): The decoded RGB mask tensor of shape (3, H, W).","metadata":{"id":"kyJCVrcESkY1"}},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass MarsSurface(Dataset):\n    \n    def __init__(self, mode: str, X, y, label_map_dict, color_map_dict, train_transforms = train_transforms, valid_transforms = valid_transforms ):\n        assert mode in [\"train\", \"valid\",\"test\", \"debug\"], \"Invalid value for self.mode, type 'train' or 'test'\"\n        self.label_map_dict = label_map_dict\n        self.color_map_dict = color_map_dict\n        self.mode = mode\n        self.transforms = train_transforms if self.mode in [\"train\", \"debug\"] else valid_transforms\n\n        self.img_size = [64, 128]\n        self.X = X\n        self.y = y\n\n    def _read_data(self, item: int):\n        image = self.X[item]\n        if image.max() > 1:\n            image = (image  / 255).astype(\"float32\")\n        label = self._decode_label(self.y[item])\n\n        return image, label\n\n    def _decode_label(self, label_map):\n        return encode_label_from_label_map(label_map, self.label_map_dict)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, item: int):\n        image, mask = self._read_data(item)\n        _label = decode_mask_from_color_map(mask, self.color_map_dict)\n\n        mask = torch.from_numpy(mask).permute(2, 0, 1).float()\n        image = torch.from_numpy(image).float()\n        _label = torch.from_numpy(_label).permute(2, 0, 1).float()\n\n        # hide values on mask which aren't annotated\n        zero_mask = 1 - mask[0,...]\n        zero_mask = torch.stack([zero_mask] * 5)\n\n        return image, mask, zero_mask, _label\n\n# Labels for decoding\nlabel_map_dict = {\n  'Terrain1': [0],\n  'Terrain2': [1],\n  'Terrain3': [2],\n  'Terrain4': [3],\n  'Terrain5': [4],\n\n}\n\n# Colors for plotting and encoding\ncolor_map_dict = {\n  'Terrain1': [0, 0, 0],\n  'Terrain2': [254, 0, 254],\n  'Terrain3': [0, 253, 0],\n  'Terrain4': [0, 0, 252],\n  'Terrain5': [250, 250, 250],\n}","metadata":{"id":"HMkbl0I2XXng","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T11:47:25.415649Z","iopub.execute_input":"2024-12-12T11:47:25.416477Z","iopub.status.idle":"2024-12-12T11:47:25.426366Z","shell.execute_reply.started":"2024-12-12T11:47:25.416440Z","shell.execute_reply":"2024-12-12T11:47:25.424928Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def create_datasets(X, y, label_map_dict, color_map_dict):\n    \n    X_train, X_val, y_train, y_val = train_test_split(\n        X,\n        y,\n        test_size = 50,\n        random_state = 7\n    )\n\n    train_dataset = MarsSurface(\n        mode = \"train\",\n        X = X_train,\n        y = y_train,\n        label_map_dict = label_map_dict,\n        color_map_dict = color_map_dict\n    )\n    \n    valid_dataset = MarsSurface(\n        mode = \"valid\",\n        X = X_val,\n        y = y_val,\n        label_map_dict = label_map_dict,\n        color_map_dict = color_map_dict\n    )\n\n    return train_dataset, valid_dataset","metadata":{"id":"XDFj1_i9XXnh","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T11:47:30.995732Z","iopub.execute_input":"2024-12-12T11:47:30.996541Z","iopub.status.idle":"2024-12-12T11:47:31.001229Z","shell.execute_reply.started":"2024-12-12T11:47:30.996508Z","shell.execute_reply":"2024-12-12T11:47:31.000156Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Losses","metadata":{"id":"hifLVS4vtxk5"}},{"cell_type":"code","source":"def dice_channel_torch(probability, truth, threshold):\n    batch_size = truth.shape[0]\n    channel_num = truth.shape[1]\n    mean_dice_channel = 0.0\n    dice_channels = [0.0] * channel_num\n    with torch.no_grad():\n        for i in range(batch_size):\n            for j in range(channel_num):\n                channel_dice = dice_single_channel(probability[i, j, :, :], truth[i, j, :, :], threshold)\n                dice_channels[j] += channel_dice / batch_size\n                mean_dice_channel += channel_dice / (batch_size * channel_num)\n    return mean_dice_channel, dice_channels\n\ndef dice_single_channel(probability, truth, threshold, eps=1e-9):\n    t = truth.flatten() > 0.5\n    p = probability.flatten() > threshold\n    dice = (2.0 * (p * t).sum() + eps) / (p.sum() + t.sum() + eps)\n    return dice\n\ndef focal_binary_cross_entropy(logits, targets, gamma=2):\n    num_label = 5\n    l = logits.reshape(-1)\n    t = targets.reshape(-1)\n    p = torch.sigmoid(l)\n    p = torch.where(t >= 0.5, p, 1-p)\n    logp = - torch.log(torch.clamp(p, 1e-4, 1-1e-4))\n    loss = logp*((1-p)**gamma)\n    loss = num_label*loss.mean()\n    return loss\n\ndef one_hot(label, n_classes, requires_grad=True):\n    \"\"\"Return One Hot Label\"\"\"\n    device = label.device\n    one_hot_label = torch.eye(\n        n_classes, device=device, requires_grad=requires_grad)[label]\n    one_hot_label = one_hot_label.transpose(1, 3).transpose(2, 3)\n\n    return one_hot_label\n\n\nclass BoundaryLoss(nn.Module):\n    \"\"\"Boundary Loss proposed in:\n    Alexey Bokhovkin et al., Boundary Loss for Remote Sensing Imagery Semantic Segmentation\n    https://arxiv.org/abs/1905.07852\n    \"\"\"\n\n    def __init__(self, theta0=3, theta=5):\n        super().__init__()\n\n        self.theta0 = theta0\n        self.theta = theta\n\n    def forward(self, pred, gt):\n        \"\"\"\n        Input:\n            - pred: the output from model (before softmax)\n                    shape (N, C, H, W)\n            - gt: ground truth map\n                    shape (N, H, w)\n        Return:\n            - boundary loss, averaged over mini-bathc\n        \"\"\"\n        print(pred.shape, gt.shape)\n        n, c, _, _ = pred.shape\n\n        # softmax so that predicted map can be distributed in [0, 1]\n        pred = torch.softmax(pred, dim=1)\n\n        # one-hot vector of ground truth\n        one_hot_gt = gt #one_hot(gt, c)\n\n        # boundary map\n        gt_b = F.max_pool2d(\n            1 - one_hot_gt, kernel_size=self.theta0, stride=1, padding=(self.theta0 - 1) // 2)\n        gt_b -= 1 - one_hot_gt\n\n        pred_b = F.max_pool2d(\n            1 - pred, kernel_size=self.theta0, stride=1, padding=(self.theta0 - 1) // 2)\n        pred_b -= 1 - pred\n\n        # extended boundary map\n        gt_b_ext = F.max_pool2d(\n            gt_b, kernel_size=self.theta, stride=1, padding=(self.theta - 1) // 2)\n\n        pred_b_ext = F.max_pool2d(\n            pred_b, kernel_size=self.theta, stride=1, padding=(self.theta - 1) // 2)\n\n        # reshape\n        gt_b = gt_b.view(n, c, -1)\n        pred_b = pred_b.view(n, c, -1)\n        gt_b_ext = gt_b_ext.view(n, c, -1)\n        pred_b_ext = pred_b_ext.view(n, c, -1)\n\n        # Precision, Recall\n        P = torch.sum(pred_b * gt_b_ext, dim=2) / (torch.sum(pred_b, dim=2) + 1e-7)\n        R = torch.sum(pred_b_ext * gt_b, dim=2) / (torch.sum(gt_b, dim=2) + 1e-7)\n\n        # Boundary F1 Score\n        BF1 = 2 * P * R / (P + R + 1e-7)\n\n        # summing BF1 Score for each class and average over mini-batch\n        loss = torch.mean(1 - BF1)\n\n        return loss","metadata":{"id":"7y_EVB4kXXnh","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T11:47:33.257125Z","iopub.execute_input":"2024-12-12T11:47:33.257964Z","iopub.status.idle":"2024-12-12T11:47:33.272598Z","shell.execute_reply.started":"2024-12-12T11:47:33.257921Z","shell.execute_reply":"2024-12-12T11:47:33.271539Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def iou(preds, labels, C, EMPTY=1.0, ignore=None, per_image=False):\n    \"\"\"\n    Array of IoU for each (non ignored) class\n    \"\"\"\n    if not per_image:\n        preds, labels = (preds,), (labels,)\n    ious = []\n    for pred, label in zip(preds, labels):\n        iou = []\n        for i in range(C):\n            if i != ignore:  # The ignored label is sometimes among predicted classes (ENet - CityScapes)\n                intersection = ((label == i) & (pred == i)).sum()\n                union = ((label == i) | ((pred == i) & (label != ignore))).sum()\n                if not union:\n                    iou.append(EMPTY)\n                else:\n                    iou.append(float(intersection) / union)\n        ious.append(iou)\n    ious = map(mean, zip(*ious))  # mean accross images if per_image\n    return 100 * np.array(ious)\n\ndef dice_loss(true, logits, eps=1e-7):\n    \"\"\"Computes the Sørensen–Dice loss.\n    Note that PyTorch optimizers minimize a loss. In this\n    case, we would like to maximize the dice loss so we\n    return the negated dice loss.\n    Args:\n        true: a tensor of shape [B, 1, H, W].\n        logits: a tensor of shape [B, C, H, W]. Corresponds to\n            the raw output or logits of the model.\n        eps: added to the denominator for numerical stability.\n    Returns:\n        dice_loss: the Sørensen–Dice loss.\n    \"\"\"\n    num_classes = logits.shape[1]\n    if num_classes == 1:\n        true_1_hot = torch.eye(num_classes + 1)[true.long().squeeze(1)]\n        true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n        true_1_hot_f = true_1_hot[:, 0:1, :, :]\n        true_1_hot_s = true_1_hot[:, 1:2, :, :]\n        true_1_hot = torch.cat([true_1_hot_s, true_1_hot_f], dim=1)\n        pos_prob = torch.sigmoid(logits)\n        neg_prob = 1 - pos_prob\n        probas = torch.cat([pos_prob, neg_prob], dim=1)\n    else:\n        # true_1_hot = torch.eye(num_classes)[true.long().squeeze(1)]\n        # true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n        true_1_hot = true\n        probas = F.softmax(logits, dim=1)\n    true_1_hot = true_1_hot.type(logits.type())\n    dims = (0,) + tuple(range(2, true.ndimension()))\n    intersection = torch.sum(probas * true_1_hot, dims)\n    cardinality = torch.sum(probas + true_1_hot, dims)\n    dice_loss = (2.0 * intersection / (cardinality + eps)).mean()\n    return 1 - dice_loss\n\n\ndef jaccard_loss(true, logits, eps=1e-7):\n    \"\"\"Computes the Jaccard loss, a.k.a the IoU loss.\n    Note that PyTorch optimizers minimize a loss. In this\n    case, we would like to maximize the jaccard loss so we\n    return the negated jaccard loss.\n    Args:\n        true: a tensor of shape [B, H, W] or [B, 1, H, W].\n        logits: a tensor of shape [B, C, H, W]. Corresponds to\n            the raw output or logits of the model.\n        eps: added to the denominator for numerical stability.\n    Returns:\n        jacc_loss: the Jaccard loss.\n    \"\"\"\n    num_classes = logits.shape[1]\n    if num_classes == 1:\n        true_1_hot = torch.eye(num_classes + 1)[true.long().squeeze(1)]\n        true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n        true_1_hot_f = true_1_hot[:, 0:1, :, :]\n        true_1_hot_s = true_1_hot[:, 1:2, :, :]\n        true_1_hot = torch.cat([true_1_hot_s, true_1_hot_f], dim=1)\n        pos_prob = torch.sigmoid(logits)\n        neg_prob = 1 - pos_prob\n        probas = torch.cat([pos_prob, neg_prob], dim=1)\n    else:\n        # true_1_hot = torch.eye(num_classes)[true.long().squeeze(1)]\n        # true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n        true_1_hot = true\n        probas = F.softmax(logits, dim=1)\n    true_1_hot = true_1_hot.type(logits.type())\n    dims = (0,) + tuple(range(2, true.ndimension()))\n    intersection = torch.sum(probas * true_1_hot, dims)\n    cardinality = torch.sum(probas + true_1_hot, dims)\n    union = cardinality - intersection\n    jacc_loss = (intersection / (union + eps)).mean()\n    return 1 - jacc_loss\n","metadata":{"id":"SjWtzZUociLQ","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T11:47:37.342815Z","iopub.execute_input":"2024-12-12T11:47:37.343177Z","iopub.status.idle":"2024-12-12T11:47:37.357295Z","shell.execute_reply.started":"2024-12-12T11:47:37.343147Z","shell.execute_reply":"2024-12-12T11:47:37.356332Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Training","metadata":{"id":"PWILM6OMtzz0"}},{"cell_type":"code","source":"def count_proportion(label, y):\n    cnt = 0\n    all_pixels = 0\n    \n    for img in y:\n        cnt += np.sum([img == label])\n        all_pixels += img.shape[0] * img.shape[1]\n    \n    return cnt / all_pixels\n\nweights = []\nfor label in label_map_dict.values():\n    weights.append(count_proportion(label, y))","metadata":{"id":"MZcchb9oXXni","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T11:47:41.294852Z","iopub.execute_input":"2024-12-12T11:47:41.295760Z","iopub.status.idle":"2024-12-12T11:47:41.588173Z","shell.execute_reply.started":"2024-12-12T11:47:41.295711Z","shell.execute_reply":"2024-12-12T11:47:41.587329Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Class `TrainingProcess`\r\n\r\nThe `TrainingProcess` class manages the training pipeline for a semantic segmentation model in PyTorch, providing methods for model initialization, data preparation, training, validation, and checkpoint saving.-\n### `__init__(self, X, y, label_map_dict, color_map_dict, weights, encoder_name, model_type = \"deeplabv3\", lr = 0.00001, model_weights=None, loss_name=None, add_name=\"\")`\r\nConstructor method that initializes the training process with various hyperparameters and settings.\r\n\r\n**Arguments:**\r\n- `X, y`: Input data and labels for training.\r\n- `label_map_dict, color_map_dict`: Dictionaries for label and color mappings.\r\n- `weights`: Class weights for loss function.\r\n- `encoder_name`: Name of the encoder for the model.\r\n- `model_type`: Type of segmentation model to use (default is \"deeplabv3\").\r\n- `lr`: Learning rate (default 0.00001).\r\n- `model_weights`: Pretrained model weights (optional).\r\n- `loss_name`: Name of the loss function to use (optional).\r\n- `add_name`: Suffix to append to the mo- save path.\r\n\r\n### `init_settings(self)`\r\nInitializes all necessary components for the training process including model, loss function, optimizer, learning rate sch-ler, and data.\r\n\r\n### `init_data(self)`\r\nPrepares training and validation datasets using the input data `X` and `y`, as well as the label and color maps. Initializes data loaders for both trai-g and validation.\r\n\r\n### `init_model(self)`\r\nInitializes the model based on the specified `model_type` (e.g., \"segformer\", \"Unet\", \"DeepLabV3Plus\", \"PAN\", \"MAnet\"). If `model_weights` is provided, it loads - pretrained weights.\r\n\r\n### `init_loss(self, name)`\r\nSets up the loss function based on the provided `name` argument (e.g., `CrossEntropyLoss`, `DiceLoss`, `foc-binary_cross_entropy`).\r\n\r\n### `_switch_data_to_device(self, list_data)`\r\nMoves the input data (either a tensor or a list/tuple of tensors) to the-rrect device (GPU or CPU).\r\n\r\n### `init_optimizer(self)`\r\nInitializes the Adam optimizer for model parameters us- the specified learning rate.\r\n\r\n### `init_lr_scheduler(self)`\r\nSets up a learning rate scheduler-ing a cosine annealing function.\r\n\r\n### `save_checkpoint(self, loss_val, path)`\r\nSaves the model checkpoint if the current validation loss -better than the previous best loss.\r\n\r\n### `run(self)`\r\nMain training loop. Trains and validates the model over the specified number of epochs, saving the best m-l and periodically saving checkpoints.\r\n\r\n### `train_step(self, epoch)`\r\nPerforms one training step, including forward pass, loss calcupagation, and optimizer update.\r\n\r\n### `val_step(self, epoch)`\r\nPerforms one validat.run()\r\n\", lr=0.00001)\r\n\r\n# Start training\r\ntrainer.run()\r\n","metadata":{}},{"cell_type":"code","source":"class TrainingProcess:\n    def __init__(self, X, y, label_map_dict, color_map_dict, weights, encoder_name, model_type=\"deeplabv3\", lr=0.00001, model_weights=None, loss_name=None, add_name=\"\"):\n        \"\"\"\n        Initializes the training process with various settings, including model type, data, loss function, optimizer, and more.\n\n        Parameters:\n        - X: Input features (training images)\n        - y: Ground truth labels (segmentation masks)\n        - label_map_dict: Dictionary for label mapping\n        - color_map_dict: Dictionary for color mapping\n        - weights: Class weights for loss calculation\n        - encoder_name: Encoder type for the model\n        - model_type: Type of segmentation model (e.g., \"DeepLabV3Plus\")\n        - lr: Learning rate for the optimizer\n        - model_weights: Pre-trained model weights (optional)\n        - loss_name: Loss function type (e.g., \"CrossEntropyLoss\")\n        - add_name: Suffix for the model save path (optional)\n        \"\"\"\n        self.label_map_dict = label_map_dict\n        self.color_map_dict = color_map_dict\n        self.encoder_name = encoder_name\n        self.num_classes = 5  # Number of classes for segmentation\n        self.lr = lr\n        self.lrf = 0.09\n        self.epochs = 250\n        self.num_workers = 1\n        self.model_type = model_type\n        self.model_weights = model_weights\n        self.batch_size = 32\n        self.X = X\n        self.y = y\n        self.iou_t = 0.5\n        self.best_loss = 10e10\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.weights = weights\n        self.save_period = 40\n        self.save_path = f\"models/{add_name}{self.model_type}_{self.encoder_name}/\"\n        os.makedirs(self.save_path, exist_ok=True)\n        self.loss_name = loss_name\n        print(f\"Lr {self.lr}\")\n        self.init_settings()\n\n    def init_settings(self):\n        \"\"\"Initializes all necessary components for the training process, such as model, loss function, optimizer, and data.\"\"\"\n        self.init_model()\n        self.init_loss(self.loss_name)\n        self.init_optimizer()\n        self.init_lr_scheduler()\n        self.init_data()\n\n    def init_data(self):\n        \"\"\"Initializes datasets and dataloaders for both training and validation.\"\"\"\n        print(f\"Initializing the data\")\n        self.train_database, self.val_database = create_datasets(self.X, self.y, self.label_map_dict, self.color_map_dict)\n        self.train_loader = DataLoader(self.train_database, batch_size=self.batch_size, num_workers=self.num_workers, drop_last=True, pin_memory=True)\n        self.val_loader = DataLoader(self.val_database, batch_size=self.batch_size * 2, num_workers=self.num_workers * 2, shuffle=False, drop_last=False, pin_memory=True)\n        self.nb = len(self.train_loader)\n\n    def init_model(self):\n        \"\"\"Initializes the model based on the selected model type.\"\"\"\n        print(f\"Initializing the model\")\n        if self.model_type == \"segformer\":\n            self.model = SegformerForSemanticSegmentation(SegformerConfig(image_size=[64,128], num_channels=3, num_labels=5)).to(self.device)\n        elif self.model_type == \"Unet\":\n            self.model = smp.Unet(encoder_name=self.encoder_name, classes=self.num_classes, encoder_weights=None, in_channels=3, activation=None).to(self.device)\n        elif self.model_type == \"DeepLabV3Plus\":\n            self.model = smp.DeepLabV3Plus(encoder_name=self.encoder_name, classes=self.num_classes, encoder_weights=None, in_channels=3, activation=None).to(self.device)\n        elif self.model_type == \"PAN\":\n            self.model = smp.PAN(encoder_name=self.encoder_name, classes=self.num_classes, encoder_weights=None, in_channels=3, activation=None).to(self.device)\n        elif self.model_type == \"MAnet\":\n            self.model = smp.MAnet(encoder_name=self.encoder_name, classes=self.num_classes, encoder_weights=None, in_channels=3, activation=None).to(self.device)\n        if self.model_weights:\n            print(f\"Init model weights from {self.model_weights}\")\n            self.model.load_state_dict(torch.load(self.model_weights, weights_only=True))\n\n    def init_loss(self, name):\n        \"\"\"Initializes the loss function based on the specified name.\"\"\"\n        print(f\"Initializing the loss\")\n        if name == \"CrossEntropyLoss\":\n            self.seg_criterion = nn.CrossEntropyLoss(weight=self.weights)\n        elif name == 'DiceLoss':\n            self.seg_criterion = smp.losses.DiceLoss(mode=\"multiclass\")\n        elif name == \"focal_binary_cross_entropy\":\n            self.seg_criterion = focal_binary_cross_entropy\n        elif name == \"BoundaryLoss\":\n            self.seg_criterion = BoundaryLoss()\n        else:\n            raise NotImplementedError\n\n    def _switch_data_to_device(self, list_data):\n        \"\"\"Moves data to the correct device (CPU or GPU).\"\"\"\n        if isinstance(list_data, (list, tuple)):\n            return [x.to(self.device, non_blocking=True) for x in list_data]\n        return list_data.to(self.device, non_blocking=True)\n\n    def init_optimizer(self):\n        \"\"\"Initializes the optimizer (Adam) for model parameters.\"\"\"\n        print(f\"Initializing the optimizer\")\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n\n    def init_lr_scheduler(self):\n        \"\"\"Initializes the learning rate scheduler with cosine annealing.\"\"\"\n        print(f\"Initializing the lr scheduler\")\n        lf = (lambda x: ((1 + math.cos(x * math.pi / self.epochs)) / 2) * (1 - self.lrf) + self.lrf)\n        self.lr_scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=lf)\n\n    def save_checkpoint(self, loss_val, path):\n        \"\"\"Saves the model checkpoint if the validation loss has improved.\"\"\"\n        if loss_val[0] <= self.best_loss:\n            self.best_loss = loss_val[0]\n            torch.save(self.model.state_dict(), path)\n            print(f\"======= save best: {path}\\n\")\n\n    def run(self):\n        \"\"\"Main training loop that runs for the specified number of epochs.\"\"\"\n        best = self.save_path + f\"best_{self.encoder_name}.pt\"\n        for epoch in range(self.epochs):\n            last = self.save_path + f\"last_{self.model_type}_{self.encoder_name}_{epoch}.pt\"\n            self.epoch = epoch\n            loss_train, metric_train = self.train_step(epoch)\n            print(f\"Epoch {epoch}/{self.epochs}, train loss {loss_train}, train metric {metric_train}\")\n            self.lr_scheduler.step()\n            print(f\"lr: {self.optimizer.param_groups[0]['lr']}\")\n            loss_val, metric_val = self.val_step(epoch)\n            print(f\"Epoch {epoch}/{self.epochs}, val loss {loss_val}, val metric {metric_val}\\n\")\n            self.save_checkpoint(loss_val, best)\n            if epoch % self.save_period == 0:\n                print(f\"======= Save last model {last}\\n\")\n                torch.save(self.model.state_dict(), last)\n            torch.cuda.empty_cache()\n\n    def train_step(self, epoch):\n        \"\"\"Performs a single training step, including forward pass, loss calculation, and optimizer update.\"\"\"\n        self.model.train()\n        self.optimizer.zero_grad(set_to_none=True)\n        self.model.zero_grad(set_to_none=True)\n        pbar = tqdm(enumerate(self.train_loader), total=len(self.train_loader), dynamic_ncols=True)\n        mloss = torch.zeros(1).to(self.device)\n        mmetric = torch.zeros(1).to(self.device)\n        for i, (inputs, masks, zero_mask, label) in pbar:\n            inputs, masks, zero_mask, label = self._switch_data_to_device([inputs, masks, zero_mask, label])\n            with torch.set_grad_enabled(True):\n                with amp.autocast(enabled=True, dtype=torch.float32):\n                    out = self.model(inputs)\n                    out_mask = out * zero_mask\n                    loss_seg = self.seg_criterion(out_mask, masks)\n                    loss = loss_seg\n                loss.backward()\n                self.optimizer.step()\n                self.optimizer.zero_grad(set_to_none=True)\n                self.model.zero_grad(set_to_none=True)\n            mloss = (mloss * i + loss.detach()) / (i + 1)\n            mmetric = (mmetric * i + torch.tensor(dice_by_channels).to(self.device)) / (i + 1)\n        return mloss, mmetric\n\n    def val_step(self, epoch):\n        \"\"\"Performs a single validation step, calculating loss and metrics.\"\"\"\n        self.model.eval()\n        pbar = tqdm(enumerate(self.val_loader), total=len(self.val_loader), dynamic_ncols=True)\n        mloss = torch.zeros(1).to(self.device)\n        mmetric = torch.zeros(1).to(self.device)\n        for i, (inputs, masks, zero_mask, label) in pbar:\n            inputs, masks, zero_mask, label = self._switch_data_to_device([inputs, masks, zero_mask, label])\n            with torch.set_grad_enabled(False):\n                with amp.autocast(enabled=True, dtype=torch.float32):\n                    out = self.model(inputs)\n                    out_mask = out * zero_mask\n                    loss_seg = self.seg_criterion(out_mask, masks)\n                    loss = loss_seg\n            mloss = (mloss * i + loss.detach()) / (i + 1)\n            mmetric = (mmetric * i + torch.tensor(dice_by_channels).to(self.device)) / (i + 1)\n        return mloss, mmetric\n","metadata":{"id":"6oxds_PqXXni","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T11:47:53.881209Z","iopub.execute_input":"2024-12-12T11:47:53.881587Z","iopub.status.idle":"2024-12-12T11:47:53.915113Z","shell.execute_reply.started":"2024-12-12T11:47:53.881556Z","shell.execute_reply":"2024-12-12T11:47:53.914219Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"weights","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z-Dg0E8nt1oH","outputId":"8e268bcc-dbbb-4da0-8e48-720fbff5a259","trusted":true,"execution":{"iopub.status.busy":"2024-12-11T13:12:46.853350Z","iopub.execute_input":"2024-12-11T13:12:46.854181Z","iopub.status.idle":"2024-12-11T13:12:46.860372Z","shell.execute_reply.started":"2024-12-11T13:12:46.854133Z","shell.execute_reply":"2024-12-11T13:12:46.859405Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"[0.24322349018605346,\n 0.3389870170964685,\n 0.23272281290527733,\n 0.18375327923920093,\n 0.0013134005729998005]"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"weights_inv = [el for el in weights]\nweights_inv = torch.tensor(weights_inv)#.cuda()\nweights_inv[4] = 5\n\nweights_inv","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RlA8AGK6XXni","outputId":"8e2501b8-c334-48e3-f59b-5760f877e006","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T11:48:01.361243Z","iopub.execute_input":"2024-12-12T11:48:01.361601Z","iopub.status.idle":"2024-12-12T11:48:01.412775Z","shell.execute_reply.started":"2024-12-12T11:48:01.361572Z","shell.execute_reply":"2024-12-12T11:48:01.412078Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"tensor([0.2432, 0.3390, 0.2327, 0.1838, 5.0000], dtype=torch.float64)"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"## Train the model","metadata":{"id":"R8bSKUNct7gj"}},{"cell_type":"code","source":"training_process = TrainingProcess(\n    X,\n    y,\n    label_map_dict,\n    color_map_dict,\n    weights = None,\n    lr = 0.001,\n    encoder_name = 'mobilenet_v2',\n    model_type = 'Unet',\n    loss_name = 'focal_binary_cross_entropy',\n    add_name = 'focal_binary_cross_entropy',\n    model_weights = None\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FiwDRQ2fXXni","outputId":"7ba6c23a-b942-4162-bcbb-003a574b93a1","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T11:48:14.231827Z","iopub.execute_input":"2024-12-12T11:48:14.232528Z","iopub.status.idle":"2024-12-12T11:48:14.731487Z","shell.execute_reply.started":"2024-12-12T11:48:14.232495Z","shell.execute_reply":"2024-12-12T11:48:14.730497Z"}},"outputs":[{"name":"stdout","text":"Lr 0.001\nInitializing the model\nInitializing the loss\nInitializing the optimizer\nInitializing the lr scheduler\nInitializing the data\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"training_process.run()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":455},"id":"3GV6Q-BmXXni","outputId":"b59088ef-56b8-4536-af74-e8a142485ab7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Eval Stacking Ensemble","metadata":{"id":"avFpg-OYRZrJ"}},{"cell_type":"code","source":"model_infer = smp.Unet(\n    encoder_name = \"mobilenet_v2\",\n    classes = 5,\n    encoder_weights = None,\n    in_channels = 3,\n    activation = None,\n)\n\nmodel_infer.load_state_dict(\n    torch.load(\"/kaggle/working/models/focal_binary_cross_entropyUnet_mobilenet_v2/best_mobilenet_v2.pt\", weights_only=True, map_location=\"cpu\")\n)\n\nmodel_infer.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:18:03.064559Z","iopub.execute_input":"2024-12-12T12:18:03.065292Z","iopub.status.idle":"2024-12-12T12:18:03.283720Z","shell.execute_reply.started":"2024-12-12T12:18:03.065243Z","shell.execute_reply":"2024-12-12T12:18:03.282784Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"data = np.load(\"/kaggle/input/ensemble-dataset-best-models/ensembled_dataset_testing (5).npz\")\nX_test_ensemble = data['dataset']\nX_test_ensemble.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:18:06.975200Z","iopub.execute_input":"2024-12-12T12:18:06.975715Z","iopub.status.idle":"2024-12-12T12:18:11.495422Z","shell.execute_reply.started":"2024-12-12T12:18:06.975670Z","shell.execute_reply":"2024-12-12T12:18:11.494344Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"X_test_ensemble = np.squeeze(X_test_ensemble)\nX_test_ensemble.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:18:14.607780Z","iopub.execute_input":"2024-12-12T12:18:14.608658Z","iopub.status.idle":"2024-12-12T12:18:14.614050Z","shell.execute_reply.started":"2024-12-12T12:18:14.608623Z","shell.execute_reply":"2024-12-12T12:18:14.613057Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"(10022, 3, 64, 128)"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"# The y argument is a placeholder.\ntest_dataset = MarsSurface(\n    'valid',\n    X_test_ensemble,\n    np.random.rand(X_test_ensemble.shape[0], X_test_ensemble.shape[1], X_test_ensemble.shape[2]),\n    label_map_dict,\n    color_map_dict\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size = 1,\n    num_workers = 1,\n    shuffle = False,\n    drop_last = False,\n    pin_memory = True,\n)\n\ninfer_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel_infer.to(infer_device)","metadata":{"id":"yBdExG30Q_RF","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:18:22.205369Z","iopub.execute_input":"2024-12-12T12:18:22.205718Z","iopub.status.idle":"2024-12-12T12:18:22.915320Z","shell.execute_reply.started":"2024-12-12T12:18:22.205685Z","shell.execute_reply":"2024-12-12T12:18:22.914525Z"},"scrolled":true},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"Unet(\n  (encoder): MobileNetV2Encoder(\n    (features): Sequential(\n      (0): Conv2dNormActivation(\n        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU6(inplace=True)\n      )\n      (1): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (2): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (3): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (4): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (5): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (6): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (7): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (8): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (9): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (10): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (11): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (12): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (13): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (14): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (15): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (16): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (17): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (18): Conv2dNormActivation(\n        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU6(inplace=True)\n      )\n    )\n  )\n  (decoder): UnetDecoder(\n    (center): Identity()\n    (blocks): ModuleList(\n      (0): DecoderBlock(\n        (conv1): Conv2dReLU(\n          (0): Conv2d(1376, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention1): Attention(\n          (attention): Identity()\n        )\n        (conv2): Conv2dReLU(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention2): Attention(\n          (attention): Identity()\n        )\n      )\n      (1): DecoderBlock(\n        (conv1): Conv2dReLU(\n          (0): Conv2d(288, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention1): Attention(\n          (attention): Identity()\n        )\n        (conv2): Conv2dReLU(\n          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention2): Attention(\n          (attention): Identity()\n        )\n      )\n      (2): DecoderBlock(\n        (conv1): Conv2dReLU(\n          (0): Conv2d(152, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention1): Attention(\n          (attention): Identity()\n        )\n        (conv2): Conv2dReLU(\n          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention2): Attention(\n          (attention): Identity()\n        )\n      )\n      (3): DecoderBlock(\n        (conv1): Conv2dReLU(\n          (0): Conv2d(80, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention1): Attention(\n          (attention): Identity()\n        )\n        (conv2): Conv2dReLU(\n          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention2): Attention(\n          (attention): Identity()\n        )\n      )\n      (4): DecoderBlock(\n        (conv1): Conv2dReLU(\n          (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention1): Attention(\n          (attention): Identity()\n        )\n        (conv2): Conv2dReLU(\n          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (attention2): Attention(\n          (attention): Identity()\n        )\n      )\n    )\n  )\n  (segmentation_head): SegmentationHead(\n    (0): Conv2d(16, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): Identity()\n    (2): Activation(\n      (activation): Identity()\n    )\n  )\n)"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"pbar = tqdm(enumerate(test_loader), total=len(test_loader), dynamic_ncols=True)\npreds = []\n\nwith torch.no_grad():\n    for i, (inputs, masks, zero_mask, label) in pbar:\n        inputs = inputs.to(infer_device)\n        out = model_infer(inputs)\n        preds.append(out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:18:28.283733Z","iopub.execute_input":"2024-12-12T12:18:28.284059Z","iopub.status.idle":"2024-12-12T12:20:14.752346Z","shell.execute_reply.started":"2024-12-12T12:18:28.284030Z","shell.execute_reply":"2024-12-12T12:20:14.751344Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 10022/10022 [01:46<00:00, 94.17it/s]\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"# Create Submission CSV file","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"hWG4ymt4R4lf","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:20:19.160557Z","iopub.execute_input":"2024-12-12T12:20:19.160896Z","iopub.status.idle":"2024-12-12T12:20:19.167180Z","shell.execute_reply.started":"2024-12-12T12:20:19.160866Z","shell.execute_reply":"2024-12-12T12:20:19.166333Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"import pandas as pd\ndef y_to_df(y) -> pd.DataFrame:\n    \"\"\"Converts segmentation predictions into a DataFrame format for Kaggle.\"\"\"\n    n_samples = len(y)\n    y_flat = y.reshape(n_samples, -1)\n    df = pd.DataFrame(y_flat)\n    df[\"id\"] = np.arange(n_samples)\n    cols = [\"id\"] + [col for col in df.columns if col != \"id\"]\n    return df[cols]","metadata":{"id":"2va1uJnOTaVm","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:20:24.449173Z","iopub.execute_input":"2024-12-12T12:20:24.449970Z","iopub.status.idle":"2024-12-12T12:20:24.454671Z","shell.execute_reply.started":"2024-12-12T12:20:24.449936Z","shell.execute_reply":"2024-12-12T12:20:24.453837Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"pred_reshaped = torch.stack(preds, dim=0).squeeze(1).permute(0,2,3,1).cpu().detach().numpy()\npred_reshaped.shape\npred_label = np.argmax(pred_reshaped, axis=-1)\npred_label.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"peFpmxkOXXnj","outputId":"3509aa2d-f5f5-40d4-bbd4-16981422ba53","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:20:27.529173Z","iopub.execute_input":"2024-12-12T12:20:27.529546Z","iopub.status.idle":"2024-12-12T12:20:31.308435Z","shell.execute_reply.started":"2024-12-12T12:20:27.529516Z","shell.execute_reply":"2024-12-12T12:20:31.307403Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"(10022, 64, 128)"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"# Create and download the csv submission file\nsubmission_filename = f\"ensemble_stacking_best.csv\"\nsubmission_df = y_to_df(pred_label)\nsubmission_df.to_csv(submission_filename, index=False)\n","metadata":{"id":"aDN1wSxwXXnj","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:20:54.547955Z","iopub.execute_input":"2024-12-12T12:20:54.548321Z","iopub.status.idle":"2024-12-12T12:21:16.163245Z","shell.execute_reply.started":"2024-12-12T12:20:54.548289Z","shell.execute_reply":"2024-12-12T12:21:16.162538Z"}},"outputs":[],"execution_count":30}]}